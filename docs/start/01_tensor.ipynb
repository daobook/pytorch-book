{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc2c08e",
   "metadata": {},
   "source": [
    "# 张量\n",
    "\n",
    "张量是一种特殊的数据结构，可以将其简单地视为数学中的张量。\n",
    "\n",
    "先载入一些库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51a5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dcb7b6",
   "metadata": {},
   "source": [
    "张量可以直接从 Python 原生对象中创建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68cd3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2], [3, 4], [5, 6]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44876404",
   "metadata": {},
   "source": [
    "或者，从 {term}`Numpy` 生成张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72e3b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fe9d44",
   "metadata": {},
   "source": [
    "或者，借助一些 torch 函数创建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88558848",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data) # 全一张量\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # 随机张量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f4e262",
   "metadata": {},
   "source": [
    "可以看看张量的样子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "248af916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ea30ac",
   "metadata": {},
   "source": [
    "通过张量的 `shape` 属性来访问张量的 形状 （沿每个轴的长度）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0090539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0dbe03",
   "metadata": {},
   "source": [
    "如果想知道张量中元素的总数，即形状的所有元素乘积，可以："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff1caa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.numel() # 即 3 * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717b35f0",
   "metadata": {},
   "source": [
    "查看张量的数据类型和其所属设备："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f22bf350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.int64, device(type='cpu'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.dtype, x_data.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056fdd98",
   "metadata": {},
   "source": [
    "## 张量运算\n",
    "\n",
    "张量有超过 100 的运算，包括算术，线性代数，矩阵操作(转置，索引，切片)，采样等（更加详细的介绍见 [`torch`](https://pytorch.org/docs/stable/torch.html)）。\n",
    "\n",
    "默认情况下，张量是在 CPU 上创建的。如果想将其移到在 GPU 上，需要借助 `.to` 方法。注意：<span class=\"w3-pale-yellow\">跨设备复制大型张量在时间和内存方面是昂贵的！</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39367d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 查看 GPU 是否可用\n",
    "if torch.cuda.is_available():\n",
    "    # 如果可用，则迁移到 GPU，并打印出来\n",
    "    x_data = x_data.to('cuda')\n",
    "    print(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afddf1c7",
   "metadata": {},
   "source": [
    "下面简单的列出一些运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "399b61ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 3, dtype=torch.float32)\n",
    "t1 = torch.cat([tensor, tensor], dim=1) # 拼接\n",
    "print(t1)\n",
    "\n",
    "y1 = tensor @ tensor.T # 矩阵乘法\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04890a",
   "metadata": {},
   "source": [
    "将结果存储到操作张量中的操作称为**就地操作**（in-place）。它们由 `_` 后缀表示。例如：`x.copy_(y)`，`x.t_()`，将会更改 `x`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "726b4e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) \n",
      "\n",
      "tensor(12)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor(7)\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4df39b",
   "metadata": {},
   "source": [
    "```{important}\n",
    "CPU 上的张量和 NumPy 数组上可以共享它们的底层内存位置，改变一个就会改变另一个。\n",
    "```\n",
    "\n",
    "比如，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a40a3826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n",
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy() # 张量转换为 NumPy\n",
    "print(f\"n: {n}\")\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51682259",
   "metadata": {},
   "source": [
    "## 节省内存\n",
    "\n",
    "运行一些操作可能会导致为新结果分配内存。例如，如果我们用 Y = X + Y，我们将取消引用 Y 指向的张量，而是指向新分配的内存处的张量。\n",
    "\n",
    "在下面的例子中，我们用 Python 的 `id()` 函数演示了这一点，它给我们提供了内存中引用对象的确切地址。运行 `Y = Y + X` 后，我们会发现 `id(Y)` 指向另一个位置。这是因为 Python 首先计算 `Y + X`，为结果分配新的内存，然后使 `Y` 指向内存中的这个新位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fa5c852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "\n",
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db08193",
   "metadata": {},
   "source": [
    "这样做是不可取的，原因有两个：首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新。其次，我们可能通过多个变量指向相同参数。如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。\n",
    "\n",
    "可以使用切片表示法将操作的结果分配给先前分配的数组，例如 `Y[:] = <expression>`。为了说明这一点，我们首先创建一个新的矩阵 `Z`，其形状与另一个 `Y` 相同，使用 `zeros_like` 来分配一个全 0 的块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef34810b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 2933842197696\n",
      "id(Z): 2933842197696\n"
     ]
    }
   ],
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a33be",
   "metadata": {},
   "source": [
    "如果在后续计算中没有重复使用 X，我们也可以使用 `X[:] = X + Y` 或 `X += Y` 来减少操作的内存开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a65d16a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33185b2a",
   "metadata": {},
   "source": [
    "当然，就地操作也是符合的。"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst,ipynb",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
