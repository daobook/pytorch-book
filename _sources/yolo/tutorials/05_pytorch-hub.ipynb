{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Hub\n",
    "\n",
    "ğŸ“š æœ¬æŒ‡å—ä»‹ç»äº†å¦‚ä½•ä» PyTorch Hub https://pytorch.org/hub/ultralytics_yolov5 ä¸­åŠ è½½ YOLOv5 ğŸš€ã€‚\n",
    "\n",
    "## ä»¥å‰çš„åšæ³•\n",
    "\n",
    "ä» **Python>=3.8** çš„ç¯å¢ƒå¼€å§‹ï¼Œå¹¶å®‰è£…äº† **PyTorch>=1.7**ï¼Œä»¥åŠ `pyyaml>=5.3` ç”¨äºè¯»å– YOLOv5 é…ç½®æ–‡ä»¶ã€‚è¦å®‰è£… PyTorch è¯·å‚è§[https://pytorch.org/get-started/locally](https://pytorch.org/get-started/locally) ã€‚ å®‰è£… YOLOv5 [requirements](https://github.com/ultralytics/yolov5/blob/master/requirements.txt)ã€‚\n",
    "\n",
    "```bash\n",
    "$ pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n",
    "```\n",
    "\n",
    "å…‹éš† [ultralytics/yolov5](https://github.com/ultralytics/yolov5) èµ„æºåº“æ˜¯ä¸éœ€è¦çš„ ğŸ˜ƒã€‚\n",
    "\n",
    "## ä½¿ç”¨ PyTorch Hub åŠ è½½ YOLOv5\n",
    "\n",
    "### ç®€å•ä¾‹å­\n",
    "\n",
    "è¿™ä¸ªä¾‹å­ä» PyTorch Hub åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„ YOLOv5s æ¨¡å‹ä½œä¸º `model`ï¼Œå¹¶ä¼ é€’ä¸€ä¸ªå›¾åƒè¿›è¡Œæ¨ç†ã€‚`'yolov5s'` æ˜¯æœ€è½»ã€æœ€å¿«çš„ YOLOv5 æ¨¡å‹ã€‚æœ‰å…³æ‰€æœ‰å¯ç”¨æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ [README](https://github.com/ultralytics/yolov5#pretrained-checkpoints)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\xinet/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-1-15 torch 1.10.0 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11264MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7266973 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# è½½å…¥æ¨¡å‹\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5x, custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 720x1280 2 persons, 2 ties\n",
      "Speed: 40.5ms pre-process, 62.0ms inference, 26.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# å›¾ç‰‡æ¥æº https://ultralytics.com/images/zidane.jpg\n",
    "img = 'images/zidane.jpg'  # or file, PIL, OpenCV, numpy, multiple\n",
    "\n",
    "# æ¨ç†æœ¬åœ°å›¾ç‰‡\n",
    "results = model(img)\n",
    "\n",
    "# ç»“æœ\n",
    "results.print()  # or .show(), .save(), .crop(), .pandas(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.49628e+02, 4.30064e+01, 1.14831e+03, 7.08739e+02, 8.76500e-01, 0.00000e+00],\n",
       "        [4.33496e+02, 4.33950e+02, 5.17908e+02, 7.15133e+02, 6.58129e-01, 2.70000e+01],\n",
       "        [1.13316e+02, 1.96360e+02, 1.09305e+03, 7.10308e+02, 5.96341e-01, 0.00000e+00],\n",
       "        [9.86140e+02, 3.04344e+02, 1.02797e+03, 4.20159e+02, 2.85011e-01, 2.70000e+01]], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.xyxy[0] # å¼ é‡ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>confidence</th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>749.628357</td>\n",
       "      <td>43.006378</td>\n",
       "      <td>1148.310181</td>\n",
       "      <td>708.739380</td>\n",
       "      <td>0.876500</td>\n",
       "      <td>0</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433.496338</td>\n",
       "      <td>433.949524</td>\n",
       "      <td>517.907959</td>\n",
       "      <td>715.133118</td>\n",
       "      <td>0.658129</td>\n",
       "      <td>27</td>\n",
       "      <td>tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>113.315674</td>\n",
       "      <td>196.359955</td>\n",
       "      <td>1093.051270</td>\n",
       "      <td>710.308350</td>\n",
       "      <td>0.596341</td>\n",
       "      <td>0</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>986.139587</td>\n",
       "      <td>304.344147</td>\n",
       "      <td>1027.974243</td>\n",
       "      <td>420.158539</td>\n",
       "      <td>0.285011</td>\n",
       "      <td>27</td>\n",
       "      <td>tie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         xmin        ymin         xmax        ymax  confidence  class    name\n",
       "0  749.628357   43.006378  1148.310181  708.739380    0.876500      0  person\n",
       "1  433.496338  433.949524   517.907959  715.133118    0.658129     27     tie\n",
       "2  113.315674  196.359955  1093.051270  710.308350    0.596341      0  person\n",
       "3  986.139587  304.344147  1027.974243  420.158539    0.285011     27     tie"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pandas().xyxy[0] # pandas ç»“æœ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®ä¾‹ç»†èŠ‚\n",
    "\n",
    "è¿™ä¸ªä¾‹å­æ˜¾ç¤ºäº†ç”¨ **PIL** å’Œ **OpenCV** å›¾åƒæºè¿›è¡Œçš„ **åˆ†æ‰¹æ¨ç†**ã€‚ç»“æœå¯ä»¥**æ‰“å°åˆ°æ§åˆ¶å°**ï¼Œ**ä¿å­˜åˆ° `runs/hub`**ï¼Œåœ¨æ”¯æŒçš„ç¯å¢ƒä¸­ **æ˜¾ç¤ºåˆ°å±å¹•ä¸Š**ï¼Œå¹¶ä½œä¸º **tensors** æˆ– **pandas** æ•°æ®å¸§è¿”å›ã€‚\n",
    "\n",
    "ä¸‹é¢çš„å›¾ç‰‡å¯ä»¥ç›´æ¥ä¸‹è½½ï¼š\n",
    "\n",
    "```python\n",
    "for f in ['zidane.jpg', 'bus.jpg']:\n",
    "   torch.hub.download_url_to_file('https://ultralytics.com/images/' + f, f)\n",
    "```\n",
    "\n",
    "æˆ–è€…ç›´æ¥ä½¿ç”¨æœ¬åœ°å›¾ç‰‡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/2: 720x1280 2 persons, 2 ties\n",
      "image 2/2: 1080x810 4 persons, 1 bus\n",
      "Speed: 37.8ms pre-process, 20.0ms inference, 3.5ms NMS per image at shape (2, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "_dir = 'images/'\n",
    "imgs = [_dir + f for f in ('zidane.jpg', 'bus.jpg')]\n",
    "\n",
    "# æ¨ç†\n",
    "results = model(imgs)\n",
    "results.print()  # or .show(), .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/2: 720x1280 2 persons, 2 ties\n",
      "image 2/2: 1080x810 4 persons, 1 bus\n",
      "Speed: 26.5ms pre-process, 13.5ms inference, 4.0ms NMS per image at shape (2, 3, 640, 640)\n",
      "Saved 2 images to \u001b[1mruns\\detect\\exp2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from cv2 import cv2\n",
    "\n",
    "img1 = Image.open('images/zidane.jpg')  # PIL image\n",
    "img2 = cv2.imread('images/bus.jpg')[:, :, ::-1]  # OpenCV image (BGR to RGB)\n",
    "imgs = [img1, img2]  # batch of images\n",
    "results = model(imgs, size=640)  # includes NMS\n",
    "# ç»“æœ\n",
    "results.print()\n",
    "results.save()  # or .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](runs/hub/exp/image1.jpg)\n",
    "![](runs/hub/exp/zidane.jpg)\n",
    "\n",
    "æ‰€æœ‰çš„æ¨ç†é€‰é¡¹è§ YOLOv5 `autoShape()` [å‰å‘æ–¹æ³•](https://github.com/ultralytics/yolov5/blob/3551b072b366989b82b3777c63ea485a99e0bf90/models/common.py#L182-L191)ã€‚\n",
    "\n",
    "### æ¨ç†è®¾ç½®\n",
    "\n",
    "æ¨ç†è®¾ç½®ï¼Œå¦‚ **ç½®ä¿¡åº¦é˜ˆå€¼**ã€NMS **IoU é˜ˆå€¼** å’Œ **ç±»** è¿‡æ»¤å™¨æ˜¯æ¨¡å‹å±æ€§ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¿®æ”¹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/2: 720x1280 3 persons, 2 ties\n",
      "image 2/2: 1080x810 3 persons, 1 bus\n",
      "Speed: 2.0ms pre-process, 21.5ms inference, 15.5ms NMS per image at shape (2, 3, 320, 320)\n"
     ]
    }
   ],
   "source": [
    "model.conf = 0.25  # confidence threshold (0-1)\n",
    "model.iou = 0.45  # NMS IoU threshold (0-1)\n",
    "model.classes = None  # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n",
    "\n",
    "results = model(imgs, size=320)  # custom inference size\n",
    "results.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¾“å…¥é€šé“\n",
    "\n",
    "è¦åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„ YOLOv5s æ¨¡å‹ï¼Œæœ‰ 4 ä¸ªè¾“å…¥é€šé“è€Œä¸æ˜¯é»˜è®¤çš„ 3 ä¸ªï¼š\n",
    "\n",
    "```python\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', channels=4)\n",
    "```\n",
    "\n",
    "åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹å°†ç”±é¢„è®­ç»ƒçš„æƒé‡ç»„æˆï¼Œ**é™¤äº†** æœ€å¼€å§‹çš„è¾“å…¥å±‚ï¼Œå®ƒçš„å½¢çŠ¶ä¸å†ä¸é¢„è®­ç»ƒçš„è¾“å…¥å±‚ç›¸åŒã€‚è¾“å…¥å±‚å°†ç»§ç»­ç”±éšæœºæƒé‡åˆå§‹åŒ–ã€‚\n",
    "\n",
    "### ç±»çš„æ•°é‡\n",
    "\n",
    "è¦åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„ YOLOv5s æ¨¡å‹ï¼Œæœ‰ 10 ä¸ªè¾“å‡ºç±»ï¼Œè€Œä¸æ˜¯é»˜è®¤çš„ 80 ä¸ªï¼š\n",
    "\n",
    "```python\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', classes=10)\n",
    "```\n",
    "\n",
    "åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹å°†ç”±é¢„è®­ç»ƒçš„æƒé‡ç»„æˆï¼Œé™¤äº†è¾“å‡ºå±‚ï¼Œè¾“å‡ºå±‚çš„å½¢çŠ¶ä¸å†ä¸é¢„è®­ç»ƒçš„è¾“å‡ºå±‚ç›¸åŒã€‚è¾“å‡ºå±‚å°†ç»§ç»­ç”±éšæœºæƒé‡åˆå§‹åŒ–ã€‚\n",
    "\n",
    "### å¼ºåˆ¶é‡æ–°åŠ è½½\n",
    "\n",
    "å¦‚æœæ‚¨åœ¨ä¸Šè¿°æ­¥éª¤ä¸­é‡åˆ°é—®é¢˜ï¼Œè®¾ç½® `force_reload=True` å¯èƒ½ä¼šæœ‰å¸®åŠ©ï¼Œå®ƒå¯ä»¥ä¸¢å¼ƒç°æœ‰çš„ç¼“å­˜å¹¶å¼ºåˆ¶ä» PyTorch Hub é‡æ–°ä¸‹è½½æœ€æ–°çš„ YOLOv5 ç‰ˆæœ¬ã€‚\n",
    "\n",
    "```python\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True)  # force reload\n",
    "```\n",
    "\n",
    "### è®­ç»ƒ\n",
    "\n",
    "è¦åŠ è½½ YOLOv5 æ¨¡å‹è¿›è¡Œè®­ç»ƒè€Œä¸æ˜¯æ¨ç†ï¼Œè¯·è®¾ç½® `autoshape=False`ã€‚è¦åŠ è½½ä¸€ä¸ªå…·æœ‰éšæœºåˆå§‹åŒ–æƒé‡çš„æ¨¡å‹ï¼ˆä»å¤´å¼€å§‹è®­ç»ƒï¼‰ï¼Œä½¿ç”¨`pretrained=False`ã€‚\n",
    "\n",
    "```python\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False)  # load pretrained\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False, pretrained=False)  # load scratch\n",
    "```\n",
    "\n",
    "### Base64 ç»“æœ\n",
    "\n",
    "ç”¨äº API æœåŠ¡ã€‚è¯¦è§ <https://github.com/ultralytics/yolov5/pull/2291> å’Œ [Flask REST API](https://github.com/ultralytics/yolov5/tree/master/utils/flask_rest_api) ä¾‹å­ã€‚\n",
    "\n",
    "```python\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "results = model(imgs)  # inference\n",
    "\n",
    "results.imgs # array of original images (as np array) passed to model for inference\n",
    "results.render()  # updates results.imgs with boxes and labels\n",
    "for img in results.imgs:\n",
    "    buffered = BytesIO()\n",
    "    img_base64 = Image.fromarray(img)\n",
    "    img_base64.save(buffered, format=\"JPEG\")\n",
    "    # base64 encoded image with results\n",
    "    res = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    print(res)\n",
    "```\n",
    "\n",
    "### JSON ç»“æœ\n",
    "\n",
    "ä¸€æ—¦ä½¿ç”¨ `.to_json()` æ–¹æ³•è½¬æ¢ä¸º `.pandas()` æ•°æ®å¸§ï¼Œç»“æœå°±å¯ä»¥ä»¥ JSON æ ¼å¼è¿”å›ã€‚JSON æ ¼å¼å¯ä»¥ä½¿ç”¨ `orient` å‚æ•°è¿›è¡Œä¿®æ”¹ã€‚è¯¦è§ pandas .to_json() [æ–‡æ¡£](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html) ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"xmin\":157.9674682617,\"ymin\":201.7145080566,\"xmax\":751.6185302734,\"ymax\":686.0665283203,\"confidence\":0.9025849104,\"class\":0,\"name\":\"person\"},{\"xmin\":770.92578125,\"ymin\":53.1617126465,\"xmax\":1117.6053466797,\"ymax\":706.3565673828,\"confidence\":0.9000060558,\"class\":0,\"name\":\"person\"},{\"xmin\":436.2830810547,\"ymin\":432.2611083984,\"xmax\":514.8706054688,\"ymax\":689.2235107422,\"confidence\":0.605379045,\"class\":27,\"name\":\"tie\"},{\"xmin\":1140.7426757812,\"ymin\":382.8111572266,\"xmax\":1274.3686523438,\"ymax\":708.5679931641,\"confidence\":0.345140636,\"class\":0,\"name\":\"person\"}]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model(imgs)  # inference\n",
    "\n",
    "results.pandas().xyxy[0].to_json(orient=\"records\")  # JSON img1 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è‡ªå®šä¹‰æ¨¡å‹\n",
    "\n",
    "è¿™ä¸ªä¾‹å­ç”¨ PyTorch Hub åŠ è½½ä¸€ä¸ªè‡ªå®šä¹‰çš„ 20 ç±» [VOC](https://github.com/ultralytics/yolov5/blob/master/data/voc.yaml) è®­ç»ƒçš„ YOLOv5s æ¨¡å‹`'best.pt'`ã€‚\n",
    "\n",
    "```python\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='path/to/best.pt')  # default\n",
    "model = torch.hub.load('path/to/yolov5', 'custom', path='path/to/best.pt', source='local')  # local repo\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
